\section{ПОСТАНОВКА ЗАДАЧИ}
Как было сказано во введении, задача оптимизации во многих случаях сводится
к нахождению значений аргументов функции, при которых она принимает 
наименьшее значение. В реальных условиях на аргументы функции всегда 
накладываются дополнительные ограничения, т.к. величину параметров в реальных
процессах нельзя увеличивать неограниченно. Но если свое наименьшее значение
функция принимает при таких значениях параметров, которые далеки от предельных,
то для процесса нахождения минимума все равно есть ограничения или нет. 
Поэтому имеет смысл рассматривать задачу безусловной минимизации, т.е. считать, 
что аргументы функции могут принимать любые значения. Такую задачу решать
значительно легче, чем задачу с ограничениями. Именно эта задача безусловной 
минимизации будет рассматриваться у данной книге.

Задача безусловной минимизации формулируется так: {\it{задана непрерывная 
функция} \func, требуется найти точку $\pmb x_{*} \in\xxxxPr$, такую, что $f(\pmb x_*)
\leqslant f(\pmb x)$ для любого $\pmb x$, $\pmb x \in \xxxxPr$.}

Формально эту задачу будем записывать в виде 
\bq\min_{x\in\xxxxPr} \func\label{1}.\eq

Минимизация с ограничениями означает, что задана функция $f$, определенная на 
множестве $\Omega$, $\Omega\in\xxxxPr$, и требуется найти точку $\pmb x_*\in\Omega$, 
такую, что $f(\pmb x_*)
\leqslant f(\pmb x)$ для любого $\pmb x$, $\pmb x \in \Omega$. Если $\pmb x_*\in
\text{int}\,\Omega$, $f(\pmb x_*)\leqslant f(\pmb x)$ при любом $\pmb x\in\Omega$ и 
алгоритм поиска минимума на каждом шаге не выходит из области $\text{int\,}\Omega$, то 
для такого алгоритма все равно, были ограничения или нет. 
В таком случае можно считать, что решалась задача безусловной минимизации. Это 
расширяет круг применимости рассматриваемых далее методов. Кроме того условная 
минимизация во многих методах сводится к поиску безусловного
минимума функции на каждом шаге алгоритма.

Задача безусловной минимизации тесно связана с решением системы уравнений. Пусть 
задана система из $n$ уравнений с $n$ неизвестными:
$$\varphi_i (x_1,x_2,\dots,x_n)=0,\quad i=1,2,\dots,n.$$
Пусть $$\pmb F(\pmb x)=\begin{pmatrix}
\varphi_i (x_1,x_2,\dots,x_1)\\
\varphi_i (x_1,x_2,\dots,x_2)\\
\vdots\\
\varphi_i (x_1,x_2,\dots,x_n)
\end{pmatrix},\quad\pmb x=\begin{pmatrix}
x_1\\x_2\\ \vdots\\x_n
\end{pmatrix}.$$
Тогда $\pmb F:\mathbb R^n\to\mathbb R^n$ и систему уравнений можно записать в виде
$$\pmb F(\pmb x)=0.$$
Создадим новую функцию
$$f(\pmb x)=\sum_{i=1}^n\left(\varphi_i (x_1,x_2,\dots,x_n)\right)^2=\|\pmb F(\pmb x)\|^2.$$
Очевидно, что точка $\pmb x_*$, являющаяся решением системы  $\pmb F(\pmb x_*)
=0$ будет служить точкой, в которой функция $f(\pmb x)$ принимает наименьшее
значение, а именно нуль. Таким образом задача решения системы уравнений сводится
к задаче нахождения точки минимума некоторой функции.

Можно и наоборот, задачу нахождения точки минимума функции свести к решению 
системы уравнений. Действительно, если функция $f$ непрерывно дифференцируемая,
то в точке минимума должны выполняться равенства 
$$\frac{\partial f}{\partial x_i}=0,\quad i=1,2,\dots,n.$$
Следовательно, точка минимума является одним из решений системы уравнений. Как
мы увидим в дальнейшем, решение задачи минимизации функции в некотором смысле
проще, чем решение системы уравнений. Поэтому решние системы уравнений часто
заменяют задачей нахождения минимума функции.

Следует отметить, что задача минимизации (\ref{1}), так как она была сформулирована,
не может быть решена численными методами. Действительно, число проделанных 
вычислений всегда конечно. Мы получаем информацию только о значениях
функции в конечном числе точек пространства $\xxxxPr$. О том, насколько малые значения
принимает функция вдали от этих точек мы ничего не знаем. Где-то далеко функция 
может принимать значения намного меньшие, чем найденные нами. Гарантировать, что
найденное значение является наименьшим, можно только при аналитическом 
исследовании свойств функции, что уже не относится к численным методам. 
Поэтому реальная задача минимизации сводится к нахождению точки локального
минимума. 

Итак, задана функция $\func$, требуется найти точку $\pmb x_*$ такую, что $f(\pmb x_*)
\leqslant f(\pmb x)$ для всех $\pmb x$ из некотрой окрестности точки $\pmb x_*$. 
Формальную запись (\ref{1}) будем далее понимать в этом смысле.

Большинство методов минимизации действуют следующим образом. Выбирается в 
качестве начального приближения точка $\pmb x_0$, $\pmb x_0\in\xxxxPr$. Затем 
последовательно на основе уже имеющейся точки $\pmb x_{i-1}$ вычисляется новое 
приближение $\pmb x_i$, $i=1,2,3,\dots$ в некотором смысле более хорошее, чем 
предыдущее. Предел, если таковой существует, к которому стремятся точки $\pmb x_i$
при $i\to\infty$ принимается за ответ к задаче. Естественно, что вычислительный 
процесс не может тянутся до бесконечности, и в качестве реального ответа берется
одна из точек последовательности $\{\pmb x_i\}$. Критерий прекращения вычислений
зависит от конкретной программной реализации метода и от точности вычислений, 
нужной пользователю.

В теории, если предела последовательности не существует, то виной этому может
быть выбранный метод или сама функция. На практике могут сказываться и другие
факторы: конечное число вычислений, конечная арифметика компьютера, критерии
останова и т. д. Функция будет \glqq плохой\grqq, т. е. метод может дать расходящуюся
последовательность $\{\pmb x_i\}$, если существует последовательность таких точек
$\{\pmb z_i\}_{i=1}^\infty$, что $\lim\limits_{i\to\infty}|\pmb z_i|=\infty$ и $f(\pmb z_{i+1})\leqslant
f(\pmb z_i)$. В качестве примера можно взять функции одного переменого, графики
которых приведены на рис.~1,2. Если для этих функций в качестве начального приближения 
$x_0$ взять точку $a$, то многие методы построят расходящуюся последовательность 
точек $\{x_i\}_{i=1}^\infty$.

Второй причиной создания методом расходящейся последовательности точек 
$\{\pmb x_i\}_{i=1}^\infty$ может служить плохой выбор точки начального приближения
$\pmb x_0$. Методы, для которых выбор начального приближения $\pmb x_0$ не влияет 
на сходимость метода, будем называть {\it глобально сходящимися}. Если же сходимость
метода можно гарантировать, только в случае, когда начальное приближение $\pmb x_0$
достаточно близко к искомой точке минимума $\pmb x_\ast$, то такой метод будем называть
{\it локально сходящимся}.  В приведенном определении, естественно, предполагается, что 
метод применяется к достаточно \glqq хорошей \grqq функции, например к функции,
для которой, для 
любого $C>0$ существует $M>0$ такое,что $|f(\pmb x)|>C$ при $\|\pmb x\|>M$.

Методы минимизации классифицируются по тому, какую информацию о минимизируемой
функции они используют. Методы {\it нулевого порядка} требуют только непрерывность
функции и в процессе вычислений используют только значения функции. Методы {\it
первого порядка} требуют, чтобы функция была непрерывно дифференцируемой и 
используют кроме значений функции значения ее производных. Методы {\it
второго порядка} соответственно требуют, чтобы функция была дважды непрерывно
дифференцируемой и используют значения функции и ее производных первого и второго
порядка.

В этой книге будут рассматриваться лишь методы второго порядка. Они предъявляют к 
минимизируемой функции более жесткие требования, но обладают тем преимуществом, что
вырабатываемая последовательность точек $\{\pmb x_i\}_{i=1}^\infty$ сходится быстрее, т.е.
в реальном вычислительном процессе придется находить меньше точек, чем в методах
первого или нулевого порядков. Методы первого и нулевого порядков автор предполагает
рассмотреть в главах, которые будут написаны позже.
\subsection{Проблемы минимизации}
Возникает естественный вопрос, зачем нужно большое число различных методов 
минимизации? Вроде бы можно было воспользоваться следующим простым, легко 
понимаемым и легко программируемым методом (этот метод реально используется).

Выделим область в $\xxxxPr$, в которой, как ожидается, находится точка минимума функции.
В практических задачах это всегда можно сделать. Для простоты будем считать, что эта
область является прямоугольным параллелепипедом $a_i\leqslant x _i\leqslant b_i$,
$i=1,2,\ldots,n$. Случай $n=2$ приведен на рис. 3. При постановке задачи пользователем
указывается желаемая точность $\ep$, с которой нужно получить ответ. Наносим на 
ось переменного $x_i$ на отрезок $[a_i;b_i]$ точки с шагом $\ep$. Через полученные точки
проводим $(n-1)$-мерные плоскости, параллельные соответствующим $(n-1)$-мерным 
координатным плоскостям. На рис. 3 --- это прямые. Получаем набор (решетку) точек, 
находящихся на 
пересечении этих плоскостей. Заставляем компьютер в цикле перебрать все эти точки, в
каждой вычислить значение функции и выбрать точку, в которой это значение наименьшее.
Ответ получен. 

Точек получится много, но компьютер считает быстро, поэтому задача
вроде бы решена. Однако проведем несложную оценку объема вычислений. Для простоты
предположим, что область, где ищем минимум, является $n$-мерным кубом с ребром 0.9.
Точность возьмем невысокую $\ep=0.1$. Тогда на каждой из осей будет отмечено 10 точек,
а точек решетки будет $10^n$. При $n=2$ или $n=3$ получается 100 или 1000 точек. В 
большинстве случаев компьютер сосчитает 1000 значений функции за долю секунды. Однако
если $n=25$ (это не много, в задачах расчета теплосетей г. Иваново $n$ порядка нескольких
тысяч) функцию придется вычислять $10^{25}$ раз. Предположим, что вычисления выполняются
на компьютере недавно введенном в действие в МГУ, самом мощном в Европе, выполняющем
$6\cdot 10^{13}$ операций в секунду. Для простоты увеличим его мощность до $10^{14}$ 
операций в секунду. Тогда этому компьютеру придется решать задачу не менее $\frac{10^{25}}
{10^{14}}=10^{11}$ секунд, что составляет более, чем 3000 лет. 
Из приведенного примера видно, что целью разработки различных методов минимизации
является уменьшение числа нахождений значения функции.

Как увидим в дальнейшем, в методах второго порядка функцию приходится вычислять
в меньшем числе точек, чем в методах более низких порядков. Но в методе второго порядка
в каждой точке $\pmb x_i$ кроме самой функции приходится вычислять $n$ первых 
производных и $n^2$ вторых производных, вычисление которых, как правило, более
трудоемкое. Таким образом при $n=100$ объем вычислений увеличивается в 
10000 раз! Это означает, что требуя одинаковый объем вычислительной работы, одну точку 
метода второго порядка можно заменить 10000 точек первого порядка.
Кроме того в методах второго порядка для каждой точки $\pmb x_i$ приходится решать
линейную систему из $n$ уравнений с $n$ неизвестными. Для решения такой системы 
требуется порядка $n^3$ вычислительных операций, что очень существенно сказывается на 
объеме работы.

Нужно также учитывать и сложность вычисления функции. В  \cite{Дэннис} говорится о
задаче, связанной со стоимостью энергии, вырабатываемой термоядерным реактором.
Минимизируемая функция зависела от 8 аргументов и для ее вычисления в каждой точке
$\pmb x_i$ приходилось решать уравнения в частных производных, а это очень 
трудоемкая задача. В той же книге говорится о задаче с нефтяной техникой, где на вычисление
одного значения функции тратилось около 100 часов работы компьютера. Данные приведены 
для 70-х годов прошлого века, сейчас компьютеры работают значительно быстрее, но и задачи
становятся соответственно сложнее.

Еще одна проблема минимизации заключается в том, где, в какой точке $\pmb x_i$, прекращать
процесс вычислений. Если останавливать вычисления, когда $\pmb x_{i+1}$ мало отличается от 
$\pmb x_i$, то возможно полученный ответ будет далек от точки локального минимума, даже 
если в теории метод должен выдать правильный ответ. Так, например, ведет себя метод 
наискорейшего спуска в случае овражных функций.

Сложности возникают и при учете точности вычисления минимизируемой функции $f(\pmb x)$.
Если эта функция задана аналитическими формулами, то компьютер будет их вычислять
со своей предельной точностью около 15 знаков. Но если функция сама является результатом
некоторого вычислительного процесса, например численного решения дифференциальных 
уравнений, то точность ограничивается обычно тремя-четырьмя знаками после запятой. В
случае таких функций применение методов первого и второго порядка осложняется
низкой точностью приближенного вычисления производных.
\subsection{Скорость сходимости}
Как было сказано ранее, каждый метод минимизации вырабатывает последовательность
точек \ih. Результат получается только тогда, когда эта последовательность сходится. 
Предельную точку обозначим \zh, $\pmb x_*=\lim\limits_{i\to\infty}\pmb x_i$. Объем
вычислительной работы будет зависеть от скорости уменьшения величины $|\pmb x_i-\zh|$.
Для характеристики этого уменьшения вводятся следующие определения скорости сходимости
последовательности точек.
\Def
Последовательность \prh\ сходится к \zh\ {\it линейно}, если существуют положительные числа
$c$, $c\in [0;1)$ и $k_0$ такие, что при всех $k>k_0$ выполняется
неравенство
\bq
\|\pmb x_{k+1}-\pmb x_*\|<c\|\pmb x_k-\pmb x_*\|.
\eq
Последовательность \prh\ сходится {\it сверхлинейно}, если существует последовательность 
положительных чисел $c_k$, $\lim\limits_{k\to\infty}c_k=0$, такая, что 
\bq
\|\pmb x_{k+1}-\pmb x_*\|<c_k\|\pmb x_k-\pmb x_*\|.
\eq
Последовательность \prh\ сходится {\it сверхлинейно с порядком} $p$, $p>1$, если существуют
$c \geqslant 0$, и $k_0>0$ такие, что при всех $k\geqslant k_0$
выполняется неравенство
\bq
\|\pmb x_{k+1}-\pmb x_*\|<c\|\pmb x_k-\pmb x_*\|^p.\label{4}
\eq
Если $p=2$, то говорят, что скорость сходимости является  {\it квадратичной}.
\Kon
\Zam
В соответствии с \cite{Дэннис} в этом определении нужно было бы использовать термины
{\it q}\/-линейно, {\it q}\/-сверхлинейно, {\it q}\/-порядок $p$, {\it q}\/-квадратичная. Такая терминология
связана с тем, что существует иная оценка скорости сходимости: {\it r}\/-линейная, 
{\it r}\/-сверхлинейная и т.д. В нашей книгее второй способ оценки скорости сходимости 
использоваться не будет, поэтому термины в определении были упрощены.
\Kon
\Zam
Очевидно, что если последовательность сходится с некоторым порядком $p_1$, $p_1>1$,
то она сходится и с любым порядком $p_2$, $p_1>p_2>1$.
\Kon
\Zam
Особенно просто иллюстрируется квадратичная сходимость при $c=1$ и $\|\pmb x_{k_0}-\zh\|
<0.1$. Тогда при замене точного предела \zh на $\pmb x_k$ при каждом следующем $k$ 
число верных знаков после запятой в десятичной записи $\pmb x_k$ будет удваиваться!
\Kon
Под скоростью сходимости метода минимизации мы будем понимать скорость сходимости
последовательности вырабатываемых им точек.

Для наглядности представления о том, как ведут себя последовательности с разной скоростью
сходимости, рассмотрим несколько примеров. Во всех примерах $x\in\mathbb R^1$,
$x_0=2$, $x_*=1$. Будем считать, что $x_j=x_*$, т.е. вычисления окончены, если $|x_j-1|<
10^{-15}$.

Последовательность 
$$x_i=1+\left(\frac{1}{2}\right)^i,\quad i=1,2,\dots$$
сходится линейно с $c=\frac{1}{2}$:
$$|x_{i+1}-1|=\left(\frac{1}{2}\right)^{i+1},$$
$$|x_i-1|=\left(\frac{1}{2}\right)^i,$$
$$|x_{i+1}-1|\leqslant\frac{1}{2}|x_i-1|.$$
В качестве ответа можно взять точку $x_{50}$.
Если взять последовательность $x_i=1+0.9^i$, то в качестве ответа придется взять $x_{328}$.
Если же $x_i=1+0.1^i$, то в качестве ответа можно взять $x_{15}$. Обе эти последовательности
сходятся линейно с константами $c=0.9$ и $c=0.1$ соответственно.

Последовательность $$x_i=1+\frac{1}{(i+1)!}$$ сходится сверхлинейно с последовательностью
констант $$c_i=\frac{1}{i+2}\,.$$ Действительно,
$$|x_i-1|=\frac{1}{(i+1)!}\,,$$
$$|x_{i+1}-1|=\frac{1}{(i+2)!}\,,$$
$$|x_{i+1}-1|=\frac{1}{(i+2)}\frac{1}{(i+1)!}=\frac{1}{i+2}|x_i-1|.$$
В качестве ответа можно взять точку $x_{17}$.

Последовательность
$$x_i=1+\left(\frac{1}{2}\right)^{2^i}$$
сходится квадратично с константой $c=1$. Действительно,
$$|x_{i+1}-1|=\left(\frac{1}{2}\right)^{2^{i+1}}=\left(\frac{1}{2}\right)^{2^i\cdot 2}=\left(\left(\frac{1}{2}
\right)^{2^i}\right)^2,$$
$$|x_i-1|=\left(\frac{1}{2}\right)^{2^i},$$
$$|x_{i+1}-1|\leqslant |x_i-1|^2.$$
В качестве ответа можно взять $x_6$.

Из приведенных примеров можно сделать вывод, что методы с квадратичной сходимостью 
являются самыми выгодными. Однако в действительности это далеко не так. При квадратичной
сходимости неравенство (\ref{4}) начинает выполняться только при $k\geqslant k_0$. Если $k_0$
велико, то количество вычисляемых членов окажется весьма большим. Аналогичное замечание
относится к последовательностям, сходящимся линейно и сверхлинейно. Следовательно, 
скорость сходимости --- это характеристика поведения метода, когда выработана точка
достаточно близкая к искомой.
\section{МЕТОД НЬЮТОНА ДЛЯ РЕШЕНИЯ СИСТЕМЫ\newline НЕЛИНЕЙНЫХ УРАВНЕНИЙ}
Метод Ньютона мы сначала рассмотрим для задачи о нахождении решения системы 
нелинейных уравнений, и затем убедимся, что задача минимизации функции методом
Ньютона сводится к решению методом Ньютона системы нелинейных уравнений.
\subsection{Смысл и алгоритм метода Ньютона}
Задана функция $\pmb F:\xxxxPr\to\xxxxPr$. Требуется найти точку $\pmb x$, $\pmb x\in\xxxxPr$,
такую что 
\bq
\pmb F(\pmb x)=0\label{5}
\eq

Обозначим через $J(\pmb x)$ матрицу Якоби функции $\pmb F(\pmb x)$, т.е.
$$J(\pmb x)=
\bm
\frac{\partial F_1}{\partial x_1} & \frac{\partial F_1}{\partial x_2}&\ldots & \frac{\partial F_1}{\partial x_n}\\
\frac{\partial F_2}{\partial x_1} & \frac{\partial F_2}{\partial x_2}&\ldots & \frac{\partial F_2}{\partial x_n}\\
\hdotsfor{4}\\
\frac{\partial F_n}{\partial x_1} & \frac{\partial F_n}{\partial x_2}&\ldots & \frac{\partial F_n}{\partial x_n}\\
\enm.$$
Пусть \xc\ (current) --- текущее приближение, т.е. это начальная точка \xz\ или последняя 
вычисленная точка \xk.
Покажем, как вычисляется следующее приближение \xp, которое в последовательности \prh 
получит номер $\pmb x_{k+1}$. 

В  точке \xc\ построим линейную модель функции $\pmb F(\pmb x)$ взяв два первых члена ряда
Тейлора этой функции в точке \xc. Модель обозначим $\pmb M_c(\pmb x)$. Тогда 
$$\pmb M_c(\pmb x)=\pmb F(\xc)+J(\xc)(\pmb x-\xc).$$
Вектор $\pmb x$ представим в виде $\pmb x=\xc+\pp$. Получим
$$\pmb M_c(\xc+\pmb p)=\pmb F(\xc)+J(\xc)\pmb p.$$
Так как $\pmb M_c(\pmb x)$ является хорошим приближением функции $\pmb F(\pmb x)$ в 
некоторой окрестности точки \xc, то вместо уравнения \slk{5}  решим уравнение 
$\pmb M_c(\pmb x)=0.$ Это векторное уравнение является линейным $\pmb F(\xc)+J(\xc)\pmb p=0$,
откуда $\pmb p=-\left(J(\xc)\right)^{-1}\pmb F(\xc)$. Найденное решение обозначим \sn, т.е \sn ---
это решение уравнения 
\bq
J(\xc)\sn=-\pmb F(\xc).\label{6}
\eq
Окончательно получим $\xp=\xc+\sn$.
\Zam Верхний индекс $N$ означает, что вектор сдвига в новую
точку вычислен по методу Ньютона. В дальнейшем сдвиги будут вычисляться, как по методу
Ньютона, так и по другим правилам, поэтому и вводится этот индекс $N$.

Итак получено 
следующее приближение.

\textbf{Алгоритм метода Ньютона} для решения систем нелинейных уравнений (\ref{5}).

Задана непрерывно дифференцируемая функция $\pmb F:\xxxxPr\to\xxxxPr$, выбрано начальное 
приближение \xz. На очередной $k$-ой итерации решается система линейных уравнений
\bq
\begin{matrix}
J(\xk)\pmb s_k^N=-\pmb F(\xk),\\
\pmb x_{k+1}=\xk+\pmb s_k^N,
\end{matrix}
\qquad k=0,1,2,\dots.
\label{7}
\eq
Для понимания метода рассмотрим случай, когда $n=1$. Тогда метод Ньютона имеет хорошую
геометричекую иллюстрацию.

Итак, $F(x)$ --- непрерывно дифференцируемая скалярная функция скалярного аргумента $x$,
$x\in \mathbb R$, $F(x)\in \mathbb R$. Ее матрица Якоби является обычной производной 
$J(x)=F'(x)$, линейная модель $$M_c(x)=F(x_c)+F'(x_c)(x-x_c)$$ имеет своим графиком касательную
к графику функции $F(x)$ в точке с абсциссой $x_c$. Приравнивая модель к нулю, мы находим
$x_{\scriptscriptstyle{+}}$ --- точку
\xp пересечения касательной с осью $Ox$, которую и принимаем за следующее 
приближение. Три итерации этого процесса приведены на рис.~4. Для случая $n=2$ 
геометрическое изображение становится сложнее, так как графики функций приходится 
изображать уже в трехмерном пространстве. Можно только наглядно показать, как 
расположена последовательность точек \ph, вырабатываемая методом, на плоскости аргументов.

Приведем пример. Пусть
$$\pmb F(\pmb x)=
\bm
x_1^2+x_2^2-3\\
x_1^2+3x_2^2-5
\enm.$$
Тогда система уравнений
$$\left\{
\begin{matrix}
x_1^2+x_2^2-3=0,\\
x_1^2+3x_2^2-5=0,
\end{matrix}
\right.$$
соответствующая векторному уравнению (\ref{5}), имеет очевидное решение
$$\zh =
\bm
\sqrt {2}\\1
\enm$$
(одно из четырех). Возьмем 
$$\pmb x _0=\bm3\\5\enm.$$
Тогда
$$\pmb x_1=\bm 1.833\\2.6\enm,\quad\pmb x_2=\bm 1.462\\1.492\enm,\quad
\pmb x_3=\bm 1.415\\1.081\enm.$$
Расположение этих точек показано на рис. 5.

